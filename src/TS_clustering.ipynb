{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prorated_imputed = pd.read_pickle(\"../output/NYCHA_Electricity_2010_to_2018_df_prorated_kwh_imputed\")\n",
    "df_prorated_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_one_building = df_prorated_imputed[df_prorated_imputed['Building_Meter']=='368.0 - BLD 02_8383053']\n",
    "df_one_building = df_prorated_imputed[df_prorated_imputed['Building_Meter']=='165.0 - BLD 03_90327795']\n",
    "#df_one_building = df_prorated_imputed[df_prorated_imputed['Building_Meter']=='165.0 - BLD 04_99273488']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_one_building = df_one_building.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sliding_chunker(data, window_len, slide_len):\n",
    "    \"\"\"\n",
    "    Split a list into a series of sub-lists, each sub-list window_len long,\n",
    "    sliding along by slide_len each time. If the list doesn't have enough\n",
    "    elements for the final sub-list to be window_len long, the remaining data\n",
    "    will be dropped.\n",
    "    e.g. sliding_chunker(range(6), window_len=3, slide_len=2)\n",
    "    gives [ [0, 1, 2], [2, 3, 4] ]\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for pos in range(0, len(data), slide_len):\n",
    "        chunk = np.copy(data[pos:pos+window_len])\n",
    "        if len(chunk) != window_len:\n",
    "            continue\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_waves(waves, step):\n",
    "    \"\"\"\n",
    "    Plot a set of 9 waves from the given set, starting from the first one\n",
    "    and increasing in index by 'step' for each subsequent graph\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    n_graph_rows = 3\n",
    "    n_graph_cols = 3\n",
    "    graph_n = 1\n",
    "    wave_n = 0\n",
    "    for _ in range(n_graph_rows):\n",
    "        for _ in range(n_graph_cols):\n",
    "            axes = plt.subplot(n_graph_rows, n_graph_cols, graph_n)\n",
    "            axes.set_ylim([min(df_one_building['Imputed_KWH'])-10000, max(df_one_building['Imputed_KWH'])+10000])\n",
    "            plt.plot(waves[wave_n])\n",
    "            graph_n += 1\n",
    "            wave_n += step\n",
    "    # fix subplot sizes so that everything fits\n",
    "    plt.suptitle('Waveform Segments of 8 data points')\n",
    "    plt.tight_layout(pad=2,h_pad=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct(data, window, clusterer):\n",
    "    \"\"\"\n",
    "    Reconstruct the given data using the cluster centers from the given\n",
    "    clusterer.\n",
    "    \"\"\"\n",
    "    window_len = len(window)\n",
    "    slide_len = window_len/2\n",
    "    segments = sliding_chunker(data, window_len, slide_len)\n",
    "    reconstructed_data = np.zeros(len(data))\n",
    "    for segment_n, segment in enumerate(segments):\n",
    "        # window the segment so that we can find it in our clusters which were\n",
    "        # formed from windowed data\n",
    "        segment *= window\n",
    "        nearest_match_idx = clusterer.predict(segment)[0]\n",
    "        nearest_match = np.copy(clusterer.cluster_centers_[nearest_match_idx])\n",
    "\n",
    "        pos = segment_n * slide_len\n",
    "        reconstructed_data[pos:pos+window_len] += nearest_match\n",
    "\n",
    "    return reconstructed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_len = 8\n",
    "slide_len = 1\n",
    "\n",
    "segments = []\n",
    "for start_pos in range(0, len(df_one_building['Imputed_KWH']), slide_len):\n",
    "    end_pos = start_pos + segment_len\n",
    "    # make a copy so changes to 'segments' doesn't modify the original data\n",
    "    segment = np.copy(df_one_building['Imputed_KWH'][start_pos:end_pos])\n",
    "    # if we're at the end and we've got a truncated segment, drop it\n",
    "    if len(segment) != segment_len:\n",
    "        continue\n",
    "    segments.append(segment)\n",
    "\n",
    "print(\"Produced %d waveform segments\" % len(segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_waves(segments, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = KMeans(n_clusters=12)\n",
    "clusterer.fit(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_waves(clusterer.cluster_centers_, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slide_len = 1\n",
    "test_segments = sliding_chunker(\n",
    "    df_one_building['Imputed_KWH'],\n",
    "    window_len=segment_len,\n",
    "    slide_len=slide_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = clusterer.cluster_centers_\n",
    "\n",
    "segment = np.copy(test_segments[16])\n",
    "# predict() returns a list of centres to cope with the possibility of multiple\n",
    "# samples being passed\n",
    "nearest_centroid_idx = clusterer.predict(test_segments[16].reshape(1,-1))[0]\n",
    "nearest_centroid = np.copy(centroids[nearest_centroid_idx])\n",
    "plt.figure()\n",
    "plt.plot(segment, label=\"Original segment\");\n",
    "plt.plot(nearest_centroid, label=\"Nearest centroid\");\n",
    "plt.title('Comparison of original and predicted at index 8');\n",
    "plt.xlabel('Index within each segment');\n",
    "plt.ylabel('Imputed KWH Consumption');\n",
    "plt.legend();\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_one_building['Imputed_KWH']\n",
    "reconstruction = np.zeros(len(data))\n",
    "\n",
    "\n",
    "for segment_n, segment in enumerate(test_segments):\n",
    "    # don't modify the data in segments\n",
    "    segment = np.copy(segment)\n",
    "    nearest_centroid_idx = clusterer.predict(segment.reshape(1,-1))[0]\n",
    "    centroids = clusterer.cluster_centers_\n",
    "    nearest_centroid = np.copy(centroids[nearest_centroid_idx])\n",
    "    \n",
    "    # overlay our reconstructed segments with an overlap of half a segment\n",
    "    pos = int(segment_n * slide_len)\n",
    "    reconstruction[pos:pos+segment_len] += nearest_centroid/(segment_len/slide_len)\n",
    "#     if 8 >= pos and 8 < pos+segment_len:\n",
    "#          plt.plot(np.linspace(0,7,8)+pos, nearest_centroid,label = pos)\n",
    "\n",
    "# fix first segment_len and last segment_len data points since they are not modeled segment_len/slide_len times\n",
    "for i in np.linspace(0,segment_len-1,segment_len).astype(int):\n",
    "    reconstruction[i] = reconstruction[i]/(i+1)*(segment_len/slide_len)\n",
    "    reconstruction[-i -1 ] = reconstruction[-i - 1]/(i+1)*(segment_len/slide_len)\n",
    "\n",
    "n_plot_samples = len(data)\n",
    "error = reconstruction[0:n_plot_samples] - data[0:n_plot_samples]\n",
    "error_99th_percentile = np.percentile(error, 99)\n",
    "print(\"Maximum reconstruction error was %.1f\" % error.max())\n",
    "print(\"99th percentile of reconstruction error was %.1f\" % error_99th_percentile)\n",
    "\n",
    "figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(data[0:n_plot_samples], label=\"Original Data\")\n",
    "plt.plot(reconstruction[0:n_plot_samples], label=\"Reconstructed Data\")\n",
    "plt.plot(np.abs(error[0:n_plot_samples]), label=\"Abs Reconstruction Error\")\n",
    "plt.title('Reconstructed Data vs. Original Data');\n",
    "plt.xlabel('Index within each account');\n",
    "plt.ylabel('Imputed KWH Consumption');\n",
    "plt.legend();\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.abs(error[0:n_plot_samples]), label=\"Reconstruction Error\")\n",
    "plt.axhline(y=error_99th_percentile,linestyle='--',color='gray');\n",
    "plt.title('Abs Reconstruction error with 99th percentile error threshold');\n",
    "plt.xlabel('Index within each account');\n",
    "plt.ylabel('Reconstruction Error');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anomaly_entries = df_one_building[np.abs(error[0:n_plot_samples])>error_99th_percentile]\n",
    "anomaly_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_one_building['Anomaly'] = np.where(np.abs(error[0:n_plot_samples])>error_99th_percentile, 'Yes', 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = df_one_building[df_one_building['Anomaly']=='Yes'][['Building_Meter','Month','Month_Type','Imputed_KWH','Anomaly']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output['Error'] = error[df_one_building['Anomaly']=='Yes']\n",
    "output['Reconstructed Value'] = reconstruction[df_one_building['Anomaly']=='Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[['Building_Meter','Month','Imputed_KWH','Reconstructed Value','Error','Anomaly']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Define function and Loop through all accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clustering_reconstruction(df_one_building):\n",
    "    \n",
    "    segment_len = 8\n",
    "    slide_len = 1\n",
    "\n",
    "    segments = []\n",
    "    for start_pos in range(0, len(df_one_building['Imputed_KWH']), slide_len):\n",
    "        end_pos = start_pos + segment_len\n",
    "        # make a copy so changes to 'segments' doesn't modify the original data\n",
    "        segment = np.copy(df_one_building['Imputed_KWH'][start_pos:end_pos])\n",
    "        # if we're at the end and we've got a truncated segment, drop it\n",
    "        if len(segment) != segment_len:\n",
    "            continue\n",
    "        segments.append(segment)\n",
    "        \n",
    "    clusterer = KMeans(n_clusters=12)\n",
    "    clusterer.fit(segments)\n",
    "        \n",
    "    data = df_one_building['Imputed_KWH']\n",
    "    reconstruction = np.zeros(len(data))\n",
    "\n",
    "\n",
    "    test_segments = sliding_chunker(\n",
    "        df_one_building['Imputed_KWH'],\n",
    "        window_len=segment_len,\n",
    "        slide_len=slide_len\n",
    "    )\n",
    "\n",
    "\n",
    "    for segment_n, segment in enumerate(test_segments):\n",
    "        # don't modify the data in segments\n",
    "        segment = np.copy(segment)\n",
    "        nearest_centroid_idx = clusterer.predict(segment.reshape(1,-1))[0]\n",
    "        centroids = clusterer.cluster_centers_\n",
    "        nearest_centroid = np.copy(centroids[nearest_centroid_idx])\n",
    "\n",
    "        # overlay our reconstructed segments with an overlap of half a segment\n",
    "        pos = int(segment_n * slide_len)\n",
    "        reconstruction[pos:pos+segment_len] += nearest_centroid/(segment_len/slide_len)\n",
    "    #     if 8 > pos and 8 < pos+segment_len:\n",
    "    #          plt.plot(np.linspace(1,8,8)+pos, nearest_centroid,label = pos)\n",
    "\n",
    "    # fix first segment_len and last segment_len data points since they are not modeled segment_len/slide_len times\n",
    "    for i in np.linspace(0,segment_len-1,segment_len).astype(int):\n",
    "        reconstruction[i] = reconstruction[i]/(i+1)*(segment_len/slide_len)\n",
    "        reconstruction[-i -1 ] = reconstruction[-i - 1]/(i+1)*(segment_len/slide_len)\n",
    "\n",
    "    n_plot_samples = len(data)\n",
    "    error = reconstruction[0:n_plot_samples] - data[0:n_plot_samples]\n",
    "    error_99th_percentile = np.percentile(error, 99)\n",
    "    \n",
    "    df_one_building['Anomaly'] = np.where(np.abs(error[0:n_plot_samples])>error_99th_percentile, 'True', 'False')\n",
    "    df_one_building['Error'] = error\n",
    "    df_one_building['Reconstructed Value'] = reconstruction\n",
    "    \n",
    "    return df_one_building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_account = pd.read_csv(\"../output/NYCHA_Accounts_More_Than_50_Months.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = []\n",
    "for account in valid_account.loc[:,'Account_Name']: \n",
    "\n",
    "    df_one_building = df_prorated_imputed[df_prorated_imputed['Building_Meter']==account]\n",
    "\n",
    "    df_one_building_result = clustering_reconstruction(df_one_building)\n",
    "    \n",
    "    result.append(df_one_building_result)\n",
    "    \n",
    "result = pd.concat(result, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_summary = pd.pivot_table(result,\n",
    "                                index = ['Building_Meter'],\n",
    "                                aggfunc = 'count',\n",
    "                                columns = 'Anomaly',\n",
    "                                values = 'Month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### clean code with comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: slice account level trend into 8 data point waveforms with sliding window of 1 step\n",
    "def sliding_chunker(data, window_len, slide_len):\n",
    "    \"\"\"\n",
    "    Split an account level trend data into waveforms,\n",
    "    each waveform is window_len long,\n",
    "    sliding along by slide_len each time.\n",
    "    If the list doesn't have enough elements for the final sub-list \n",
    "    to be window_len long, the remaining data will be dropped.\n",
    "    e.g. sliding_chunker(range(6), window_len=3, slide_len=2)\n",
    "    gives [ [0, 1, 2], [2, 3, 4] ]\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for pos in range(0, len(data), slide_len):\n",
    "        chunk = np.copy(data[pos:pos+window_len])\n",
    "        if len(chunk) != window_len:\n",
    "            continue\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: cluster the chucks into 12 clusters using KMeans clsutering and reconstruct by taking mean of centroids\n",
    "def clustering_reconstruction(df_one_building, segment_len = 8, slide_len = 1):\n",
    "    \"\"\"\n",
    "    This functions consists of two main parts: Clustering and Reconstruction.\n",
    "    The Clustering part clusters segments from slicer into 12 clusters.\n",
    "    The Reconstruction part stitches all centroids mean by original data index.\n",
    "    \"\"\"\n",
    "\n",
    "    segments = []\n",
    "    for start_pos in range(0, len(df_one_building['Imputed_KWH']), slide_len):\n",
    "        end_pos = start_pos + segment_len\n",
    "        # make a copy so changes to 'segments' doesn't modify the original data\n",
    "        segment = np.copy(df_one_building['Imputed_KWH'][start_pos:end_pos])\n",
    "        # if we're at the end and we've got a truncated segment, drop it\n",
    "        if len(segment) != segment_len:\n",
    "            continue\n",
    "        segments.append(segment)\n",
    "        \n",
    "    # use KMeans function from sklearn to cluster segments into 12 clusters representing each month\n",
    "    clusterer = KMeans(n_clusters=12)\n",
    "    clusterer.fit(segments)\n",
    "    \n",
    "    # define data for reconstruction \n",
    "    data = df_one_building['Imputed_KWH']\n",
    "    reconstruction = np.zeros(len(data))\n",
    "\n",
    "\n",
    "    test_segments = sliding_chunker(\n",
    "        df_one_building['Imputed_KWH'],\n",
    "        window_len=segment_len,\n",
    "        slide_len=slide_len\n",
    "    )\n",
    "\n",
    "\n",
    "    for segment_n, segment in enumerate(test_segments):\n",
    "        segment = np.copy(segment)\n",
    "        nearest_centroid_idx = clusterer.predict(segment.reshape(1,-1))[0]\n",
    "        centroids = clusterer.cluster_centers_\n",
    "        nearest_centroid = np.copy(centroids[nearest_centroid_idx])\n",
    "\n",
    "        # overlay our reconstructed segments with an overlap of half a segment\n",
    "        pos = int(segment_n * slide_len)\n",
    "        reconstruction[pos:pos+segment_len] += nearest_centroid/(segment_len/slide_len)\n",
    "\n",
    "    # fix first segment_len and last segment_len data points since they are not modeled segment_len/slide_len times\n",
    "    for i in np.linspace(0,segment_len-1,segment_len).astype(int):\n",
    "        reconstruction[i] = reconstruction[i]/(i+1)*(segment_len/slide_len)\n",
    "        reconstruction[-i -1 ] = reconstruction[-i - 1]/(i+1)*(segment_len/slide_len)\n",
    "\n",
    "    error = reconstruction[0:len(data)] - data[0:len(data)]\n",
    "    error_99th_percentile = np.percentile(error, 99)\n",
    "    \n",
    "    df_one_building['Anomaly'] = np.where(np.abs(error[0:len(data)])>error_99th_percentile, 'True', 'False')\n",
    "    df_one_building['Error'] = error\n",
    "    df_one_building['Reconstructed Value'] = reconstruction\n",
    "    \n",
    "    return df_one_building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the two inputs are 1) prorated and imputed dataframe and 2) valid account defined by users\n",
    "df_prorated_imputed = pd.read_pickle(\"../output/NYCHA_Electricity_2010_to_2018_df_prorated_kwh_imputed\")\n",
    "valid_account = pd.read_csv(\"../output/NYCHA_Accounts_More_Than_50_Months.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crystal-pro/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/crystal-pro/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/crystal-pro/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Step 3: loop over all valid accounts\n",
    "result = []\n",
    "for account in valid_account.loc[:,'Account_Name']: \n",
    "\n",
    "    df_one_building = df_prorated_imputed[df_prorated_imputed['Building_Meter']==account]\n",
    "\n",
    "    df_one_building_result = clustering_reconstruction(df_one_building)\n",
    "    \n",
    "    result.append(df_one_building_result)\n",
    "    \n",
    "result = pd.concat(result, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
