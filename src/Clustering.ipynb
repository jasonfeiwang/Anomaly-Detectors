{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective and Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes in a csv file (e.g. NYCHA_TS.csv) as input with **Building_Meter**, **Month**, **Value** as necessary column names and output a dataframe with a column (**Anomaly**) identifying whether the individual records is an anomaly point or not, a column (**Reconstructed_Value**) showing the reconstructed value from cluster centroilds ,and finally, a column (**Reconstruction_Error**) denoting the reconstruction error calculated by the absolute different between Original Value and Reconstructed Value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We require pandas and numpy for dataframe, row, column, and cell manipulation; and we require KMeans from sklearn cluster package to calculated centroids for each chunk of waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main user defined functions, one for slicing the account level time series data into waveforms of 8 data point, and another one for calculating clusters of waveforms and the centroilds for each cluster and stitching all centroids together to reconstruct the time series trend without anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: slice account level trend into 8 data point waveforms with sliding window of 1 step\n",
    "def sliding_chunker(data, window_len, slide_len):\n",
    "    \"\"\"\n",
    "    Split an account level trend data into waveforms,\n",
    "    each waveform is window_len long,\n",
    "    sliding along by slide_len each time.\n",
    "    If the list doesn't have enough elements for the final sub-list \n",
    "    to be window_len long, the remaining data will be dropped.\n",
    "    e.g. sliding_chunker(range(6), window_len=3, slide_len=2)\n",
    "    gives [ [0, 1, 2], [2, 3, 4] ]\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for pos in range(0, len(data), slide_len):\n",
    "        chunk = np.copy(data[pos:pos+window_len])\n",
    "        if len(chunk) != window_len:\n",
    "            continue\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: cluster the chucks into 12 clusters using KMeans clsutering and reconstruct by taking mean of centroids\n",
    "def clustering_reconstruction(df_one_building, segment_len = 8, slide_len = 1):\n",
    "    \"\"\"\n",
    "    This functions consists of two main parts: Clustering and Reconstruction.\n",
    "    The Clustering part clusters segments from slicer into 12 clusters.\n",
    "    The Reconstruction part stitches all centroids mean by original data index.\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    for start_pos in range(0, len(df_one_building['Value']), slide_len):\n",
    "        end_pos = start_pos + segment_len\n",
    "        # make a copy so changes to 'segments' doesn't modify the original data\n",
    "        segment = np.copy(df_one_building['Value'][start_pos:end_pos])\n",
    "        # if we're at the end and we've got a truncated segment, drop it\n",
    "        if len(segment) != segment_len:\n",
    "            continue\n",
    "        segments.append(segment)\n",
    "        \n",
    "    # use KMeans function from sklearn to cluster segments into 12 clusters representing each month\n",
    "    clusterer = KMeans(n_clusters=12)\n",
    "    clusterer.fit(segments)\n",
    "    \n",
    "    # define data for reconstruction \n",
    "    data = df_one_building['Value']\n",
    "    reconstruction = np.zeros(len(data))\n",
    "\n",
    "    # define test segments for calculating clusters\n",
    "    test_segments = sliding_chunker(\n",
    "        df_one_building['Value'],\n",
    "        window_len=segment_len,\n",
    "        slide_len=slide_len\n",
    "    )\n",
    "\n",
    "    # loop through each test segments to find the nearest centroids\n",
    "    for segment_n, segment in enumerate(test_segments):\n",
    "        segment = np.copy(segment)\n",
    "        nearest_centroid_idx = clusterer.predict(segment.reshape(1,-1))[0]\n",
    "        centroids = clusterer.cluster_centers_\n",
    "        nearest_centroid = np.copy(centroids[nearest_centroid_idx])\n",
    "\n",
    "        # overlay our reconstructed segments with an overlap of half a segment\n",
    "        pos = int(segment_n * slide_len)\n",
    "        reconstruction[pos:pos+segment_len] += nearest_centroid/(segment_len/slide_len)\n",
    "\n",
    "    # fix first segment_len and last segment_len data points since they are not modeled segment_len/slide_len times\n",
    "    for i in np.linspace(0,segment_len-1,segment_len).astype(int):\n",
    "        reconstruction[i] = reconstruction[i]/(i+1)*(segment_len/slide_len)\n",
    "        reconstruction[-i -1 ] = reconstruction[-i - 1]/(i+1)*(segment_len/slide_len)\n",
    "\n",
    "    # calculate the reconstruction errors by taking the absolute difference between reconstruct data and original data\n",
    "    error = reconstruction[0:len(data)] - data[0:len(data)]\n",
    "    error_99th_percentile = np.percentile(error, 99)\n",
    "    \n",
    "    # assign three new columns for output\n",
    "    df_one_building['Anomaly'] = np.where(np.abs(error[0:len(data)])>error_99th_percentile, 'True', 'False')\n",
    "    df_one_building['Reconstruction_Error'] = error\n",
    "    df_one_building['Reconstructed_Value'] = reconstruction\n",
    "    \n",
    "    return df_one_building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for All Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crystal-pro/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/crystal-pro/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/crystal-pro/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 30s, sys: 682 ms, total: 5min 31s\n",
      "Wall time: 5min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Step 3: loop over all accounts\n",
    "\n",
    "# Input and change all NA values to 0 for processing\n",
    "all_valid_account_data = pd.read_csv(\"../output/NYCHA_TS.csv\")\n",
    "all_valid_account_data = all_valid_account_data[['Account','Month','Value']]\n",
    "all_valid_account_data = all_valid_account_data.fillna(0)\n",
    "\n",
    "# define an empty dataframe result to store data on individual account level\n",
    "result = []\n",
    "for account in pd.unique(all_valid_account_data['Account']): \n",
    "\n",
    "    df_one_building = all_valid_account_data[all_valid_account_data['Account']==account]\n",
    "\n",
    "    df_one_building_result = clustering_reconstruction(df_one_building)\n",
    "    \n",
    "    result.append(df_one_building_result)\n",
    "    \n",
    "result = pd.concat(result, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can now save the result dataframe to desired directory in desired format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
